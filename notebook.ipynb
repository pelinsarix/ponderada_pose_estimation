{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 1: Configura√ß√µes iniciais e imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Configura√ß√µes globais\n",
    "BASE_DIR = './'\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"antelope\")\n",
    "ANNOTATIONS_FILE = os.path.join(BASE_DIR, \"antelope.json\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train_data\")\n",
    "YOLO_CONFIG = os.path.join(BASE_DIR, \"antelope_pose.yaml\")\n",
    "TRAIN_MODEL_PATH = os.path.join(BASE_DIR, \"yolov8n-pose.pt\")\n",
    "TRAINED_MODEL_PATH = os.path.join(BASE_DIR, \"yolov8n-pose-antelope.pt\")\n",
    "\n",
    "# Criar diret√≥rios necess√°rios\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configura√ß√£o YAML criada em: ./antelope_pose.yaml\n"
     ]
    }
   ],
   "source": [
    "def create_yolo_config():\n",
    "    config = f\"\"\"\n",
    "# Caminhos\n",
    "path: {BASE_DIR}\n",
    "train: {TRAIN_DIR}\n",
    "val: {TRAIN_DIR}  # Na pr√°tica usar conjunto diferente\n",
    "\n",
    "# Keypoints\n",
    "kpt_shape: [8, 3]  # 8 keypoints com (x, y, visibility)\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: antelope\n",
    "\n",
    "# Par√¢metros obrigat√≥rios\n",
    "nc: 1  # n√∫mero de classes\n",
    "flip_idx: [1, 0, 3, 2, 5, 4, 7, 6]  # conex√µes para flip horizontal\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(YOLO_CONFIG, 'w') as f:\n",
    "        f.write(config)\n",
    "        \n",
    "    print(f\"Configura√ß√£o YAML criada em: {YOLO_CONFIG}\")\n",
    "\n",
    "create_yolo_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dataset YOLO em ./train_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo anota√ß√µes:   0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo anota√ß√µes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [00:01<00:00, 90.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convers√£o conclu√≠da!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 3: Prepara√ß√£o do dataset no formato YOLO\n",
    "def prepare_yolo_dataset():\n",
    "    \"\"\"Converte anota√ß√µes para o formato YOLOv8 pose\"\"\"\n",
    "    with open(ANNOTATIONS_FILE) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    print(f\"Preparando dataset YOLO em {TRAIN_DIR}...\")\n",
    "    \n",
    "    for img_name, bboxes in tqdm(annotations.items(), desc=\"Convertendo anota√ß√µes\"):\n",
    "        try:\n",
    "            # Copiar imagem\n",
    "            src_img = os.path.join(IMAGE_DIR, img_name)\n",
    "            dst_img = os.path.join(TRAIN_DIR, img_name)\n",
    "            shutil.copy(src_img, dst_img)\n",
    "            \n",
    "            # Criar arquivo de anota√ß√µes\n",
    "            txt_path = os.path.join(TRAIN_DIR, os.path.splitext(img_name)[0] + '.txt')\n",
    "            \n",
    "            with open(txt_path, 'w') as f_txt:\n",
    "                for bbox in bboxes:\n",
    "                    img = Image.open(src_img)\n",
    "                    img_w, img_h = img.size\n",
    "                    \n",
    "                    # Converter bounding box\n",
    "                    x_center = (bbox['bndbox']['xmin'] + bbox['bndbox']['xmax']) / (2 * img_w)\n",
    "                    y_center = (bbox['bndbox']['ymin'] + bbox['bndbox']['ymax']) / (2 * img_h)\n",
    "                    width = (bbox['bndbox']['xmax'] - bbox['bndbox']['xmin']) / img_w\n",
    "                    height = (bbox['bndbox']['ymax'] - bbox['bndbox']['ymin']) / img_h\n",
    "                    \n",
    "                    # Gerar keypoints fict√≠cios (substituir por seus dados reais)\n",
    "                    keypoints = [\n",
    "                        0.7, 0.2, 2,  # snout\n",
    "                        0.6, 0.15, 2,  # head\n",
    "                        0.5, 0.3, 2,   # neck_base\n",
    "                        0.3, 0.7, 2,   # front_right_leg\n",
    "                        0.7, 0.7, 2,   # front_left_leg\n",
    "                        0.2, 0.8, 2,   # hind_right_leg\n",
    "                        0.8, 0.8, 2,   # hind_left_leg\n",
    "                        0.1, 0.4, 2    # tail_base\n",
    "                    ]\n",
    "                    \n",
    "                    line = [0, x_center, y_center, width, height] + keypoints\n",
    "                    f_txt.write(' '.join(map(str, line)) + '\\n')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {img_name}: {e}\")\n",
    "    \n",
    "    print(\"Convers√£o conclu√≠da!\")\n",
    "\n",
    "prepare_yolo_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando treinamento com transfer learning...\n",
      "New https://pypi.org/project/ultralytics/8.3.99 available  Update with 'pip install -U ultralytics'\n",
      "WARNING  'label_smoothing' is deprecated and will be removed in in the future.\n",
      "Ultralytics 8.3.92  Python-3.11.0 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=pose, mode=train, model=./yolov8n-pose.pt, data=./antelope_pose.yaml, epochs=30, time=None, patience=4, batch=8, imgsz=256, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=./results, name=antelope_pose, exist_ok=True, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=5, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=results\\antelope_pose\n",
      "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[8, 3]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    865915  ultralytics.nn.modules.head.Pose             [1, [8, 3], [64, 128, 256]]   \n",
      "YOLOv8n-pose summary: 144 layers, 3,125,451 parameters, 3,125,435 gradients, 8.6 GFLOPs\n",
      "\n",
      "Transferred 361/397 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Inteli\\Downloads\\ponderada\\train_data.cache... 152 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Inteli\\Downloads\\ponderada\\train_data.cache... 152 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to results\\antelope_pose\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
      "Image sizes 256 train, 256 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mresults\\antelope_pose\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G      1.641      9.464     0.6932      1.717      1.552         18        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:38<00:00,  2.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.544      0.507      0.497      0.248          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G      1.482      8.947     0.6774      1.327      1.388         33        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:13<00:00,  1.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.705      0.634      0.678      0.314    0.00501    0.00939   6.43e-05   1.03e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G      1.349      8.612     0.6896      1.165      1.355         26        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:11<00:00,  1.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.838      0.799      0.858      0.513     0.0286     0.0282    0.00596   0.000609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G      1.438      8.784     0.6783      1.172      1.394         21        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:11<00:00,  1.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.825      0.821      0.853      0.539    0.00787    0.00469   0.000357   7.15e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G      1.327      8.603     0.6616       1.07      1.338         29        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:12<00:00,  1.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.808      0.826      0.871      0.578     0.0252     0.0188    0.00431   0.000431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30         0G      1.322      8.448     0.6687      1.053      1.319         23        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:13<00:00,  1.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.852      0.798      0.887      0.569     0.0636     0.0469    0.00616    0.00072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30         0G      1.307      8.169     0.6764      1.047      1.329         28        256: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:11<00:00,  1.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213       0.89      0.812        0.9      0.631       0.15     0.0751     0.0267    0.00367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30         0G      1.386      8.624      0.677      1.058      1.353         31        256:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 16/19 [00:18<00:03,  1.07s/it]"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"Executa o treinamento com transfer learning\"\"\"\n",
    "    print(\"\\nIniciando treinamento com transfer learning...\")\n",
    "    \n",
    "    # Verificar dados\n",
    "    if not os.path.exists(TRAIN_DIR) or len(os.listdir(TRAIN_DIR)) == 0:\n",
    "        raise FileNotFoundError(\"Dados de treino n√£o encontrados!\")\n",
    "    \n",
    "    # Carregar modelo\n",
    "    model = YOLO(TRAIN_MODEL_PATH)\n",
    "    \n",
    "    # Hiperpar√¢metros atualizados para YOLOv8.3+\n",
    "    train_args = {\n",
    "        'data': YOLO_CONFIG,\n",
    "        'epochs': 30,\n",
    "        'imgsz': 256,\n",
    "        'batch': 8,\n",
    "        'optimizer': 'Adam',\n",
    "        'lr0': 0.001,\n",
    "        'pretrained': True,\n",
    "        'project': RESULTS_DIR,\n",
    "        'name': 'antelope_pose',\n",
    "        'exist_ok': True,\n",
    "        'device': '0' if torch.cuda.is_available() else 'cpu',\n",
    "        'freeze': 5,  # Congelar primeiras 5 camadas\n",
    "        'weight_decay': 0.0005,\n",
    "        'patience': 4,  # Early stopping\n",
    "        'label_smoothing': 0.1,\n",
    "        'fliplr': 0.5,  # Aumento de dados\n",
    "        'mosaic': 1.0,   # Aumento de dados\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Treinar modelo\n",
    "        results = model.train(**train_args)\n",
    "        \n",
    "        # Salvar modelo\n",
    "        model.save(TRAINED_MODEL_PATH)\n",
    "        \n",
    "        # Plotar m√©tricas\n",
    "        results.plot()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'training_metrics.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Treinamento conclu√≠do! Modelo salvo em: {TRAINED_MODEL_PATH}\")\n",
    "        \n",
    "        # Mostrar estat√≠sticas\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nPar√¢metros trein√°veis: {trainable_params/total_params:.2%}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante o treinamento: {e}\")\n",
    "        print(\"Solu√ß√µes potenciais:\")\n",
    "        print(\"1. Verifique o formato das anota√ß√µes nos arquivos .txt\")\n",
    "        print(\"2. Atualize o Ultralytics: pip install --upgrade ultralytics\")\n",
    "        print(\"3. Verifique o caminho absoluto das imagens no YAML\")\n",
    "        raise\n",
    "\n",
    "# Executar treinamento\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando valida√ß√£o do modelo...\n",
      "Ultralytics 8.3.92  Python-3.11.0 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv8n-pose summary (fused): 81 layers, 3,120,107 parameters, 0 gradients, 8.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Inteli\\Downloads\\ponderada\\train_data.cache... 152 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:05<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.969      0.906      0.948      0.813      0.189      0.127       0.11     0.0237\n",
      "Speed: 0.1ms preprocess, 19.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\pose\\val\u001b[0m\n",
      "‚úÖ Valida√ß√£o conclu√≠da! Precis√£o mAP50-95: 0.81\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 5: Valida√ß√£o do Modelo\n",
    "def validate_model(model):\n",
    "    \"\"\"Avalia o modelo no conjunto de valida√ß√£o\"\"\"\n",
    "    print(\"\\nIniciando valida√ß√£o do modelo...\")\n",
    "    \n",
    "    metrics = model.val(\n",
    "        data=YOLO_CONFIG,\n",
    "        batch=4,\n",
    "        imgsz=256,\n",
    "        conf=0.5,\n",
    "        iou=0.6,\n",
    "        device='0' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Salvar m√©tricas\n",
    "    with open(os.path.join(RESULTS_DIR, 'validation_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics.results_dict, f)\n",
    "    \n",
    "    print(f\"‚úÖ Valida√ß√£o conclu√≠da! Precis√£o mAP50-95: {metrics.box.map:.2f}\")\n",
    "    return metrics\n",
    "\n",
    "# Executar valida√ß√£o se o modelo existe\n",
    "if os.path.exists(TRAINED_MODEL_PATH):\n",
    "    model = YOLO(TRAINED_MODEL_PATH)\n",
    "    val_metrics = validate_model(model)\n",
    "else:\n",
    "    print(\"Modelo treinado n√£o encontrado. Pulando valida√ß√£o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executando infer√™ncia em imagens de ./test_images...\n",
      "\n",
      "image 1/3 c:\\Users\\Inteli\\Downloads\\ponderada\\test_images\\image.png: 192x256 1 antelope, 76.6ms\n",
      "image 2/3 c:\\Users\\Inteli\\Downloads\\ponderada\\test_images\\image2.png: 192x256 2 antelopes, 83.8ms\n",
      "WARNING  Image Read Error c:\\Users\\Inteli\\Downloads\\ponderada\\test_images\\image3.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Executar infer√™ncia\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(TRAINED_MODEL_PATH):\n\u001b[1;32m---> 27\u001b[0m     inference_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo treinado n√£o encontrado. Pulando infer√™ncia.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(model, source_dir)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExecutando infer√™ncia em imagens de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./inference_results\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./inference_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Gerar visualiza√ß√£o interativa\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:214\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 57\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:319\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 319\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:153\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    151\u001b[0m not_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_tensor:\n\u001b[1;32m--> 153\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:422\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    420\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    424\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"Executa o treinamento com transfer learning\"\"\"\n",
    "    print(\"\\nIniciando treinamento com transfer learning...\")\n",
    "    \n",
    "    # Verificar dados\n",
    "    if not os.path.exists(TRAIN_DIR) or len(os.listdir(TRAIN_DIR)) == 0:\n",
    "        raise FileNotFoundError(\"Dados de treino n√£o encontrados!\")\n",
    "    \n",
    "    # Carregar modelo\n",
    "    model = YOLO(TRAIN_MODEL_PATH)\n",
    "    \n",
    "    # Congelar backbone\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'backbone' in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Hiperpar√¢metros com early stopping\n",
    "    train_args = {\n",
    "        'data': YOLO_CONFIG,\n",
    "        'epochs': 30,\n",
    "        'imgsz': 256,\n",
    "        'batch': 8,\n",
    "        'optimizer': 'Adam',\n",
    "        'lr0': 0.001,\n",
    "        'pretrained': True,\n",
    "        'project': RESULTS_DIR,\n",
    "        'name': 'antelope_pose_frozen_backbone',\n",
    "        'exist_ok': True,\n",
    "        'device': '0' if torch.cuda.is_available() else 'cpu',\n",
    "        'freeze': [0, 1, 2, 3, 4],\n",
    "        'weight_decay': 0.0005,\n",
    "        'momentum': 0.9,\n",
    "        'label_smoothing': 0.1,\n",
    "        'patience': 4,\n",
    "        'save_period': 1\n",
    "    }\n",
    "    \n",
    "    # Callback corrigido\n",
    "    class CustomCallback:\n",
    "        def __init__(self):\n",
    "            self.best_map = 0.0\n",
    "            \n",
    "        def on_train_epoch_end(self, trainer):\n",
    "            current_map = trainer.validator.metrics.ap50\n",
    "            print(f\"\\n√âpoca {trainer.epoch + 1}/{trainer.args.epochs}\")\n",
    "            print(f\"mAP50: {current_map:.3f} | Best mAP50: {self.best_map:.3f}\")\n",
    "            \n",
    "            if current_map > self.best_map:\n",
    "                self.best_map = current_map\n",
    "                print(\"üéØ Novo melhor modelo encontrado!\")\n",
    "    \n",
    "    # Treinar\n",
    "    try:\n",
    "        results = model.train(\n",
    "            **train_args,\n",
    "            callbacks={'on_train_epoch_end': CustomCallback().on_train_epoch_end}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o treinamento: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Salvar modelo\n",
    "    model.save(TRAINED_MODEL_PATH)\n",
    "    \n",
    "    # Plotar m√©tricas corrigido\n",
    "    if results:\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        plt.plot(results.metrics['train/box_loss'], label='Train Loss')\n",
    "        plt.plot(results.metrics['val/box_loss'], label='Val Loss')\n",
    "        plt.title('Curva de Aprendizado')\n",
    "        plt.xlabel('√âpoca')\n",
    "        plt.ylabel('Perda')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'training_metrics_frozen.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Treinamento conclu√≠do! Modelo salvo em: {TRAINED_MODEL_PATH}\")\n",
    "    \n",
    "    # Mostrar par√¢metros trein√°veis\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nPar√¢metros totais: {total_params:,}\")\n",
    "    print(f\"Par√¢metros trein√°veis: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Executar treinamento\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando an√°lise comparativa...\n",
      "‚úÖ An√°lise comparativa conclu√≠da!\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 7: An√°lise Comparativa\n",
    "def compare_models():\n",
    "    \"\"\"Compara o modelo treinado com o modelo base\"\"\"\n",
    "    print(\"\\nIniciando an√°lise comparativa...\")\n",
    "    \n",
    "    # Carregar ambos os modelos\n",
    "    base_model = YOLO(TRAIN_MODEL_PATH)\n",
    "    trained_model = YOLO(TRAINED_MODEL_PATH)\n",
    "    \n",
    "    # M√©tricas de compara√ß√£o\n",
    "    comparison = {\n",
    "        'base_model': {\n",
    "            'params': sum(p.numel() for p in base_model.parameters()),\n",
    "            'size_mb': os.path.getsize(TRAIN_MODEL_PATH) / (1024 * 1024)\n",
    "        },\n",
    "        'trained_model': {\n",
    "            'params': sum(p.numel() for p in trained_model.parameters()),\n",
    "            'size_mb': os.path.getsize(TRAINED_MODEL_PATH) / (1024 * 1024)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Plotar compara√ß√£o\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Tamanho do modelo\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Base', 'Treinado'], \n",
    "            [comparison['base_model']['size_mb'], comparison['trained_model']['size_mb']])\n",
    "    plt.title('Compara√ß√£o de Tamanho dos Modelos')\n",
    "    plt.ylabel('Tamanho (MB)')\n",
    "    \n",
    "    # N√∫mero de par√¢metros\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['Base', 'Treinado'], \n",
    "            [comparison['base_model']['params'], comparison['trained_model']['params']])\n",
    "    plt.title('N√∫mero de Par√¢metros')\n",
    "    plt.ylabel('Par√¢metros (milh√µes)')\n",
    "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'model_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"‚úÖ An√°lise comparativa conclu√≠da!\")\n",
    "    return comparison\n",
    "\n",
    "model_comparison = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando para deployment...\n",
      "Ultralytics 8.3.92  Python-3.11.0 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n-pose-antelope.pt' with input shape (1, 3, 256, 256) BCHW and output shape(s) (1, 29, 1344) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim', 'onnxruntime'] not found, attempting AutoUpdate...\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 8: Deploy do Modelo\n",
    "def prepare_for_deployment(model):\n",
    "    \"\"\"Prepara o modelo para produ√ß√£o\"\"\"\n",
    "    print(\"\\nPreparando para deployment...\")\n",
    "    \n",
    "    # Exportar para formatos diferentes\n",
    "    model.export(format='onnx', imgsz=256, simplify=True)\n",
    "    model.export(format='engine', imgsz=256)\n",
    "    \n",
    "    # Criar arquivo de configura√ß√£o\n",
    "    deploy_config = {\n",
    "        'model_path': TRAINED_MODEL_PATH,\n",
    "        'input_size': 256,\n",
    "        'class_names': ['antelope'],\n",
    "        'keypoints': [\n",
    "            'snout', 'head', 'neck_base',\n",
    "            'front_right_leg', 'front_left_leg',\n",
    "            'hind_right_leg', 'hind_left_leg',\n",
    "            'tail_base'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(BASE_DIR, 'deploy_config.json'), 'w') as f:\n",
    "        json.dump(deploy_config, f, indent=4)\n",
    "    \n",
    "    print(\"‚úÖ Modelo pronto para deployment!\")\n",
    "    print(f\"Formatos dispon√≠veis: {os.listdir(os.path.dirname(TRAINED_MODEL_PATH))}\")\n",
    "    \n",
    "prepare_for_deployment(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
