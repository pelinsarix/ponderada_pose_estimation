{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Configurações iniciais e imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Configurações globais\n",
    "BASE_DIR = './'\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"antelope\")\n",
    "ANNOTATIONS_FILE = os.path.join(BASE_DIR, \"antelope.json\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train_data\")\n",
    "YOLO_CONFIG = os.path.join(BASE_DIR, \"antelope_pose.yaml\")\n",
    "TRAIN_MODEL_PATH = os.path.join(BASE_DIR, \"yolov8n-pose.pt\")\n",
    "TRAINED_MODEL_PATH = os.path.join(BASE_DIR, \"yolov8n-pose-antelope.pt\")\n",
    "\n",
    "# Criar diretórios necessários\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração YAML criada em: ./antelope_pose.yaml\n"
     ]
    }
   ],
   "source": [
    "def create_yolo_config():\n",
    "    config = f\"\"\"\n",
    "# Caminhos\n",
    "path: {BASE_DIR}\n",
    "train: {TRAIN_DIR}\n",
    "val: {TRAIN_DIR}  # Na prática usar conjunto diferente\n",
    "\n",
    "# Keypoints\n",
    "kpt_shape: [8, 3]  # 8 keypoints com (x, y, visibility)\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: antelope\n",
    "\n",
    "# Parâmetros obrigatórios\n",
    "nc: 1  # número de classes\n",
    "flip_idx: [1, 0, 3, 2, 5, 4, 7, 6]  # conexões para flip horizontal\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(YOLO_CONFIG, 'w') as f:\n",
    "        f.write(config)\n",
    "        \n",
    "    print(f\"Configuração YAML criada em: {YOLO_CONFIG}\")\n",
    "\n",
    "create_yolo_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dataset YOLO em ./train_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo anotações:   0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo anotações: 100%|██████████| 152/152 [00:01<00:00, 90.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversão concluída!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Célula 3: Preparação do dataset no formato YOLO\n",
    "def prepare_yolo_dataset():\n",
    "    \"\"\"Converte anotações para o formato YOLOv8 pose\"\"\"\n",
    "    with open(ANNOTATIONS_FILE) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    print(f\"Preparando dataset YOLO em {TRAIN_DIR}...\")\n",
    "    \n",
    "    for img_name, bboxes in tqdm(annotations.items(), desc=\"Convertendo anotações\"):\n",
    "        try:\n",
    "            # Copiar imagem\n",
    "            src_img = os.path.join(IMAGE_DIR, img_name)\n",
    "            dst_img = os.path.join(TRAIN_DIR, img_name)\n",
    "            shutil.copy(src_img, dst_img)\n",
    "            \n",
    "            # Criar arquivo de anotações\n",
    "            txt_path = os.path.join(TRAIN_DIR, os.path.splitext(img_name)[0] + '.txt')\n",
    "            \n",
    "            with open(txt_path, 'w') as f_txt:\n",
    "                for bbox in bboxes:\n",
    "                    img = Image.open(src_img)\n",
    "                    img_w, img_h = img.size\n",
    "                    \n",
    "                    # Converter bounding box\n",
    "                    x_center = (bbox['bndbox']['xmin'] + bbox['bndbox']['xmax']) / (2 * img_w)\n",
    "                    y_center = (bbox['bndbox']['ymin'] + bbox['bndbox']['ymax']) / (2 * img_h)\n",
    "                    width = (bbox['bndbox']['xmax'] - bbox['bndbox']['xmin']) / img_w\n",
    "                    height = (bbox['bndbox']['ymax'] - bbox['bndbox']['ymin']) / img_h\n",
    "                    \n",
    "                    # Gerar keypoints fictícios (substituir por seus dados reais)\n",
    "                    keypoints = [\n",
    "                        0.7, 0.2, 2,  # snout\n",
    "                        0.6, 0.15, 2,  # head\n",
    "                        0.5, 0.3, 2,   # neck_base\n",
    "                        0.3, 0.7, 2,   # front_right_leg\n",
    "                        0.7, 0.7, 2,   # front_left_leg\n",
    "                        0.2, 0.8, 2,   # hind_right_leg\n",
    "                        0.8, 0.8, 2,   # hind_left_leg\n",
    "                        0.1, 0.4, 2    # tail_base\n",
    "                    ]\n",
    "                    \n",
    "                    line = [0, x_center, y_center, width, height] + keypoints\n",
    "                    f_txt.write(' '.join(map(str, line)) + '\\n')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {img_name}: {e}\")\n",
    "    \n",
    "    print(\"Conversão concluída!\")\n",
    "\n",
    "prepare_yolo_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando treinamento com transfer learning...\n",
      "New https://pypi.org/project/ultralytics/8.3.99 available  Update with 'pip install -U ultralytics'\n",
      "WARNING  'label_smoothing' is deprecated and will be removed in in the future.\n",
      "Ultralytics 8.3.92  Python-3.11.0 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=pose, mode=train, model=./yolov8n-pose.pt, data=./antelope_pose.yaml, epochs=30, time=None, patience=4, batch=8, imgsz=256, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=./results, name=antelope_pose, exist_ok=True, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=5, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=results\\antelope_pose\n",
      "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[8, 3]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    865915  ultralytics.nn.modules.head.Pose             [1, [8, 3], [64, 128, 256]]   \n",
      "YOLOv8n-pose summary: 144 layers, 3,125,451 parameters, 3,125,435 gradients, 8.6 GFLOPs\n",
      "\n",
      "Transferred 361/397 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Inteli\\Downloads\\ponderada\\train_data.cache... 152 images, 0 backgrounds, 0 corrupt: 100%|██████████| 152/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Inteli\\Downloads\\ponderada\\train_data.cache... 152 images, 0 backgrounds, 0 corrupt: 100%|██████████| 152/152 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to results\\antelope_pose\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
      "Image sizes 256 train, 256 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mresults\\antelope_pose\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G      1.641      9.464     0.6932      1.717      1.552         18        256: 100%|██████████| 19/19 [00:38<00:00,  2.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:06<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.544      0.507      0.497      0.248          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G      1.482      8.947     0.6774      1.327      1.388         33        256: 100%|██████████| 19/19 [00:13<00:00,  1.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:05<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.705      0.634      0.678      0.314    0.00501    0.00939   6.43e-05   1.03e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G      1.349      8.612     0.6896      1.165      1.355         26        256: 100%|██████████| 19/19 [00:11<00:00,  1.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:05<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.838      0.799      0.858      0.513     0.0286     0.0282    0.00596   0.000609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G      1.438      8.784     0.6783      1.172      1.394         21        256: 100%|██████████| 19/19 [00:11<00:00,  1.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:05<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.825      0.821      0.853      0.539    0.00787    0.00469   0.000357   7.15e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G      1.327      8.603     0.6616       1.07      1.338         29        256: 100%|██████████| 19/19 [00:12<00:00,  1.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.808      0.826      0.871      0.578     0.0252     0.0188    0.00431   0.000431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30         0G      1.322      8.448     0.6687      1.053      1.319         23        256: 100%|██████████| 19/19 [00:13<00:00,  1.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.852      0.798      0.887      0.569     0.0636     0.0469    0.00616    0.00072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30         0G      1.307      8.169     0.6764      1.047      1.329         28        256: 100%|██████████| 19/19 [00:11<00:00,  1.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:06<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213       0.89      0.812        0.9      0.631       0.15     0.0751     0.0267    0.00367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30         0G      1.386      8.624      0.677      1.058      1.353         31        256:  84%|████████▍ | 16/19 [00:18<00:03,  1.07s/it]"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"Executa o treinamento com transfer learning\"\"\"\n",
    "    print(\"\\nIniciando treinamento com transfer learning...\")\n",
    "    \n",
    "    # Verificar dados\n",
    "    if not os.path.exists(TRAIN_DIR) or len(os.listdir(TRAIN_DIR)) == 0:\n",
    "        raise FileNotFoundError(\"Dados de treino não encontrados!\")\n",
    "    \n",
    "    # Carregar modelo\n",
    "    model = YOLO(TRAIN_MODEL_PATH)\n",
    "    \n",
    "    # Hiperparâmetros atualizados para YOLOv8.3+\n",
    "    train_args = {\n",
    "        'data': YOLO_CONFIG,\n",
    "        'epochs': 30,\n",
    "        'imgsz': 256,\n",
    "        'batch': 8,\n",
    "        'optimizer': 'Adam',\n",
    "        'lr0': 0.001,\n",
    "        'pretrained': True,\n",
    "        'project': RESULTS_DIR,\n",
    "        'name': 'antelope_pose',\n",
    "        'exist_ok': True,\n",
    "        'device': '0' if torch.cuda.is_available() else 'cpu',\n",
    "        'freeze': 5,  # Congelar primeiras 5 camadas\n",
    "        'weight_decay': 0.0005,\n",
    "        'patience': 4,  # Early stopping\n",
    "        'label_smoothing': 0.1,\n",
    "        'fliplr': 0.5,  # Aumento de dados\n",
    "        'mosaic': 1.0,   # Aumento de dados\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Treinar modelo\n",
    "        results = model.train(**train_args)\n",
    "        \n",
    "        # Salvar modelo\n",
    "        model.save(TRAINED_MODEL_PATH)\n",
    "        \n",
    "        # Plotar métricas\n",
    "        results.plot()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'training_metrics.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✅ Treinamento concluído! Modelo salvo em: {TRAINED_MODEL_PATH}\")\n",
    "        \n",
    "        # Mostrar estatísticas\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nParâmetros treináveis: {trainable_params/total_params:.2%}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro durante o treinamento: {e}\")\n",
    "        print(\"Soluções potenciais:\")\n",
    "        print(\"1. Verifique o formato das anotações nos arquivos .txt\")\n",
    "        print(\"2. Atualize o Ultralytics: pip install --upgrade ultralytics\")\n",
    "        print(\"3. Verifique o caminho absoluto das imagens no YAML\")\n",
    "        raise\n",
    "\n",
    "# Executar treinamento\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando validação do modelo...\n",
      "Ultralytics 8.3.92  Python-3.11.0 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv8n-pose summary (fused): 81 layers, 3,120,107 parameters, 0 gradients, 8.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Inteli\\Downloads\\ponderada\\train_data.cache... 152 images, 0 backgrounds, 0 corrupt: 100%|██████████| 152/152 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 38/38 [00:05<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        152        213      0.969      0.906      0.948      0.813      0.189      0.127       0.11     0.0237\n",
      "Speed: 0.1ms preprocess, 19.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\pose\\val\u001b[0m\n",
      "✅ Validação concluída! Precisão mAP50-95: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Célula 5: Validação do Modelo\n",
    "def validate_model(model):\n",
    "    \"\"\"Avalia o modelo no conjunto de validação\"\"\"\n",
    "    print(\"\\nIniciando validação do modelo...\")\n",
    "    \n",
    "    metrics = model.val(\n",
    "        data=YOLO_CONFIG,\n",
    "        batch=4,\n",
    "        imgsz=256,\n",
    "        conf=0.5,\n",
    "        iou=0.6,\n",
    "        device='0' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Salvar métricas\n",
    "    with open(os.path.join(RESULTS_DIR, 'validation_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics.results_dict, f)\n",
    "    \n",
    "    print(f\"✅ Validação concluída! Precisão mAP50-95: {metrics.box.map:.2f}\")\n",
    "    return metrics\n",
    "\n",
    "# Executar validação se o modelo existe\n",
    "if os.path.exists(TRAINED_MODEL_PATH):\n",
    "    model = YOLO(TRAINED_MODEL_PATH)\n",
    "    val_metrics = validate_model(model)\n",
    "else:\n",
    "    print(\"Modelo treinado não encontrado. Pulando validação.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executando inferência em imagens de ./test_images...\n",
      "\n",
      "image 1/3 c:\\Users\\Inteli\\Downloads\\ponderada\\test_images\\image.png: 192x256 1 antelope, 76.6ms\n",
      "image 2/3 c:\\Users\\Inteli\\Downloads\\ponderada\\test_images\\image2.png: 192x256 2 antelopes, 83.8ms\n",
      "WARNING  Image Read Error c:\\Users\\Inteli\\Downloads\\ponderada\\test_images\\image3.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Executar inferência\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(TRAINED_MODEL_PATH):\n\u001b[1;32m---> 27\u001b[0m     inference_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo treinado não encontrado. Pulando inferência.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(model, source_dir)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExecutando inferência em imagens de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./inference_results\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./inference_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Gerar visualização interativa\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:214\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 57\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:319\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 319\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:153\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    151\u001b[0m not_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_tensor:\n\u001b[1;32m--> 153\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:422\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    420\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    424\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"Executa o treinamento com transfer learning\"\"\"\n",
    "    print(\"\\nIniciando treinamento com transfer learning...\")\n",
    "    \n",
    "    # Verificar dados\n",
    "    if not os.path.exists(TRAIN_DIR) or len(os.listdir(TRAIN_DIR)) == 0:\n",
    "        raise FileNotFoundError(\"Dados de treino não encontrados!\")\n",
    "    \n",
    "    # Carregar modelo\n",
    "    model = YOLO(TRAIN_MODEL_PATH)\n",
    "    \n",
    "    # Congelar backbone\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'backbone' in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Hiperparâmetros com early stopping\n",
    "    train_args = {\n",
    "        'data': YOLO_CONFIG,\n",
    "        'epochs': 30,\n",
    "        'imgsz': 256,\n",
    "        'batch': 8,\n",
    "        'optimizer': 'Adam',\n",
    "        'lr0': 0.001,\n",
    "        'pretrained': True,\n",
    "        'project': RESULTS_DIR,\n",
    "        'name': 'antelope_pose_frozen_backbone',\n",
    "        'exist_ok': True,\n",
    "        'device': '0' if torch.cuda.is_available() else 'cpu',\n",
    "        'freeze': [0, 1, 2, 3, 4],\n",
    "        'weight_decay': 0.0005,\n",
    "        'momentum': 0.9,\n",
    "        'label_smoothing': 0.1,\n",
    "        'patience': 4,\n",
    "        'save_period': 1\n",
    "    }\n",
    "    \n",
    "    # Callback corrigido\n",
    "    class CustomCallback:\n",
    "        def __init__(self):\n",
    "            self.best_map = 0.0\n",
    "            \n",
    "        def on_train_epoch_end(self, trainer):\n",
    "            current_map = trainer.validator.metrics.ap50\n",
    "            print(f\"\\nÉpoca {trainer.epoch + 1}/{trainer.args.epochs}\")\n",
    "            print(f\"mAP50: {current_map:.3f} | Best mAP50: {self.best_map:.3f}\")\n",
    "            \n",
    "            if current_map > self.best_map:\n",
    "                self.best_map = current_map\n",
    "                print(\"🎯 Novo melhor modelo encontrado!\")\n",
    "    \n",
    "    # Treinar\n",
    "    try:\n",
    "        results = model.train(\n",
    "            **train_args,\n",
    "            callbacks={'on_train_epoch_end': CustomCallback().on_train_epoch_end}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o treinamento: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Salvar modelo\n",
    "    model.save(TRAINED_MODEL_PATH)\n",
    "    \n",
    "    # Plotar métricas corrigido\n",
    "    if results:\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        plt.plot(results.metrics['train/box_loss'], label='Train Loss')\n",
    "        plt.plot(results.metrics['val/box_loss'], label='Val Loss')\n",
    "        plt.title('Curva de Aprendizado')\n",
    "        plt.xlabel('Época')\n",
    "        plt.ylabel('Perda')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'training_metrics_frozen.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Treinamento concluído! Modelo salvo em: {TRAINED_MODEL_PATH}\")\n",
    "    \n",
    "    # Mostrar parâmetros treináveis\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nParâmetros totais: {total_params:,}\")\n",
    "    print(f\"Parâmetros treináveis: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Executar treinamento\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando análise comparativa...\n",
      "✅ Análise comparativa concluída!\n"
     ]
    }
   ],
   "source": [
    "# Célula 7: Análise Comparativa\n",
    "def compare_models():\n",
    "    \"\"\"Compara o modelo treinado com o modelo base\"\"\"\n",
    "    print(\"\\nIniciando análise comparativa...\")\n",
    "    \n",
    "    # Carregar ambos os modelos\n",
    "    base_model = YOLO(TRAIN_MODEL_PATH)\n",
    "    trained_model = YOLO(TRAINED_MODEL_PATH)\n",
    "    \n",
    "    # Métricas de comparação\n",
    "    comparison = {\n",
    "        'base_model': {\n",
    "            'params': sum(p.numel() for p in base_model.parameters()),\n",
    "            'size_mb': os.path.getsize(TRAIN_MODEL_PATH) / (1024 * 1024)\n",
    "        },\n",
    "        'trained_model': {\n",
    "            'params': sum(p.numel() for p in trained_model.parameters()),\n",
    "            'size_mb': os.path.getsize(TRAINED_MODEL_PATH) / (1024 * 1024)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Plotar comparação\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Tamanho do modelo\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Base', 'Treinado'], \n",
    "            [comparison['base_model']['size_mb'], comparison['trained_model']['size_mb']])\n",
    "    plt.title('Comparação de Tamanho dos Modelos')\n",
    "    plt.ylabel('Tamanho (MB)')\n",
    "    \n",
    "    # Número de parâmetros\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['Base', 'Treinado'], \n",
    "            [comparison['base_model']['params'], comparison['trained_model']['params']])\n",
    "    plt.title('Número de Parâmetros')\n",
    "    plt.ylabel('Parâmetros (milhões)')\n",
    "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'model_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"✅ Análise comparativa concluída!\")\n",
    "    return comparison\n",
    "\n",
    "model_comparison = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando para deployment...\n",
      "Ultralytics 8.3.92  Python-3.11.0 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n-pose-antelope.pt' with input shape (1, 3, 256, 256) BCHW and output shape(s) (1, 29, 1344) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim', 'onnxruntime'] not found, attempting AutoUpdate...\n"
     ]
    }
   ],
   "source": [
    "# Célula 8: Deploy do Modelo\n",
    "def prepare_for_deployment(model):\n",
    "    \"\"\"Prepara o modelo para produção\"\"\"\n",
    "    print(\"\\nPreparando para deployment...\")\n",
    "    \n",
    "    # Exportar para formatos diferentes\n",
    "    model.export(format='onnx', imgsz=256, simplify=True)\n",
    "    model.export(format='engine', imgsz=256)\n",
    "    \n",
    "    # Criar arquivo de configuração\n",
    "    deploy_config = {\n",
    "        'model_path': TRAINED_MODEL_PATH,\n",
    "        'input_size': 256,\n",
    "        'class_names': ['antelope'],\n",
    "        'keypoints': [\n",
    "            'snout', 'head', 'neck_base',\n",
    "            'front_right_leg', 'front_left_leg',\n",
    "            'hind_right_leg', 'hind_left_leg',\n",
    "            'tail_base'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(BASE_DIR, 'deploy_config.json'), 'w') as f:\n",
    "        json.dump(deploy_config, f, indent=4)\n",
    "    \n",
    "    print(\"✅ Modelo pronto para deployment!\")\n",
    "    print(f\"Formatos disponíveis: {os.listdir(os.path.dirname(TRAINED_MODEL_PATH))}\")\n",
    "    \n",
    "prepare_for_deployment(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
